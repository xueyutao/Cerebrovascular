{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 1 dataset...\n",
      "(20774, 2)\n",
      "labels: tensor([3, 0, 0,  ..., 0, 0, 0])\n",
      "features tensor([[-0.1469,  0.0886, -0.1448,  ...,  0.2986, -0.0217, -0.1078],\n",
      "        [-0.1469,  0.0531, -0.1447,  ...,  0.1480, -0.0582, -0.0195],\n",
      "        [-0.1469,  0.1144, -0.1448,  ...,  0.2070, -0.0337, -0.0422],\n",
      "        ...,\n",
      "        [-0.1469,  0.1209, -0.1452,  ...,  0.2070, -0.0337, -0.0422],\n",
      "        [-0.1469,  0.0886, -0.1448,  ...,  0.2070, -0.0337, -0.0422],\n",
      "        [-0.1501,  0.1338, -0.1449,  ...,  0.2070, -0.0337, -0.0422]])\n",
      "01_OK\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "import time\n",
    "import argparse\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from sklearn import metrics\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from utils import load_data2, accuracy\n",
    "from model.models import GCN\n",
    "\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei']  # 用来正确显示中文标签\n",
    "plt.rcParams['font.serif'] = ['SimHei']\n",
    "plt.rcParams['axes.unicode_minus'] = False  # 用来正确显示负号\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "adj, features, labels, idx_train, idx_val, idx_test = load_data2()\n",
    "\n",
    "print('01_OK')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM(\n",
      "  (layer1): LSTM(86, 64, num_layers=2)\n",
      "  (layer2): Linear(in_features=64, out_features=4, bias=True)\n",
      ")\n",
      "---------------\n",
      "torch.int64\n",
      "torch.Size([5764, 86])\n",
      "5764\n",
      "tensor([[[1., 1., 1., 1.]]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[[1., 1., 1., 1.]]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[[1., 1., 1., 1.]]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[[1., 1., 1., 1.]]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[[1., 1., 1., 1.]]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[[1., 1., 1., 1.]]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[[1., 1., 1., 1.]]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[[1., 1., 1., 1.]]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[[1., 1., 1., 1.]]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[[1., 1., 1., 1.]]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[[1., 1., 1., 1.]]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[[1., 1., 1., 1.]]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[[1., 1., 1., 1.]]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[[1., 1., 1., 1.]]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[[1., 1., 1., 1.]]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[[1., 1., 1., 1.]]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[[1., 1., 1., 1.]]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[[1., 1., 1., 1.]]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[[1., 1., 1., 1.]]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[[1., 1., 1., 1.]]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[[1., 1., 1., 1.]]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[[1., 1., 1., 1.]]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[[1., 1., 1., 1.]]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[[1., 1., 1., 1.]]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[[1., 1., 1., 1.]]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[[1., 1., 1., 1.]]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[[1., 1., 1., 1.]]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[[1., 1., 1., 1.]]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[[1., 1., 1., 1.]]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[[1., 1., 1., 1.]]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[[1., 1., 1., 1.]]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[[1., 1., 1., 1.]]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[[1., 1., 1., 1.]]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[[1., 1., 1., 1.]]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[[1., 1., 1., 1.]]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[[1., 1., 1., 1.]]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[[1., 1., 1., 1.]]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[[1., 1., 1., 1.]]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[[1., 1., 1., 1.]]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[[1., 1., 1., 1.]]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[[1., 1., 1., 1.]]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[[1., 1., 1., 1.]]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[[1., 1., 1., 1.]]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[[1., 1., 1., 1.]]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[[1., 1., 1., 1.]]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[[1., 1., 1., 1.]]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[[1., 1., 1., 1.]]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[[1., 1., 1., 1.]]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[[1., 1., 1., 1.]]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[[1., 1., 1., 1.]]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[[1., 1., 1., 1.]]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[[1., 1., 1., 1.]]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[[1., 1., 1., 1.]]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[[1., 1., 1., 1.]]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[[1., 1., 1., 1.]]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[[1., 1., 1., 1.]]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[[1., 1., 1., 1.]]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[[1., 1., 1., 1.]]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[[1., 1., 1., 1.]]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[[1., 1., 1., 1.]]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[[1., 1., 1., 1.]]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[[1., 1., 1., 1.]]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[[1., 1., 1., 1.]]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[[1., 1., 1., 1.]]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[[1., 1., 1., 1.]]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[[1., 1., 1., 1.]]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[[1., 1., 1., 1.]]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[[1., 1., 1., 1.]]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[[1., 1., 1., 1.]]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[[1., 1., 1., 1.]]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[[1., 1., 1., 1.]]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[[1., 1., 1., 1.]]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[[1., 1., 1., 1.]]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[[1., 1., 1., 1.]]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[[1., 1., 1., 1.]]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[[1., 1., 1., 1.]]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[[1., 1., 1., 1.]]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[[1., 1., 1., 1.]]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[[1., 1., 1., 1.]]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[[1., 1., 1., 1.]]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[[1., 1., 1., 1.]]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[[1., 1., 1., 1.]]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[[1., 1., 1., 1.]]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[[1., 1., 1., 1.]]], grad_fn=<SoftmaxBackward>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-23-6bbda0ba0be2>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     23\u001B[0m         \u001B[0mx\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mnp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mexpand_dims\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mx\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0maxis\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     24\u001B[0m         \u001B[0mx\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mTensor\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mx\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 25\u001B[1;33m         \u001B[0moutput\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mmodel\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mx\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     26\u001B[0m         \u001B[0moutput\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mF\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msoftmax\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0moutput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdim\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     27\u001B[0m         \u001B[1;31m# output = nn.Softmax(output, dim=1)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\ProgramData\\Anaconda3\\envs\\tensorflow1\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001B[0m in \u001B[0;36m__call__\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m    530\u001B[0m             \u001B[0mresult\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_slow_forward\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    531\u001B[0m         \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 532\u001B[1;33m             \u001B[0mresult\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mforward\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    533\u001B[0m         \u001B[1;32mfor\u001B[0m \u001B[0mhook\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_forward_hooks\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mvalues\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    534\u001B[0m             \u001B[0mhook_result\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mhook\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mresult\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mF:\\Cerebrovascular\\model\\LSTM.py\u001B[0m in \u001B[0;36mforward\u001B[1;34m(self, x)\u001B[0m\n\u001B[0;32m     42\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     43\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0mforward\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mx\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 44\u001B[1;33m         \u001B[0mx\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0m_\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mlayer1\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mx\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     45\u001B[0m         \u001B[0ms\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mb\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mh\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mx\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msize\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     46\u001B[0m         \u001B[0mx\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mx\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mview\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0ms\u001B[0m \u001B[1;33m*\u001B[0m \u001B[0mb\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mh\u001B[0m\u001B[1;33m)\u001B[0m  \u001B[1;31m# view函数调整矩阵的形状，类似于reshape\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\ProgramData\\Anaconda3\\envs\\tensorflow1\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001B[0m in \u001B[0;36m__call__\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m    530\u001B[0m             \u001B[0mresult\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_slow_forward\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    531\u001B[0m         \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 532\u001B[1;33m             \u001B[0mresult\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mforward\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    533\u001B[0m         \u001B[1;32mfor\u001B[0m \u001B[0mhook\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_forward_hooks\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mvalues\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    534\u001B[0m             \u001B[0mhook_result\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mhook\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mresult\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\ProgramData\\Anaconda3\\envs\\tensorflow1\\lib\\site-packages\\torch\\nn\\modules\\rnn.py\u001B[0m in \u001B[0;36mforward\u001B[1;34m(self, input, hx)\u001B[0m\n\u001B[0;32m    557\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mbatch_sizes\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    558\u001B[0m             result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n\u001B[1;32m--> 559\u001B[1;33m                               self.dropout, self.training, self.bidirectional, self.batch_first)\n\u001B[0m\u001B[0;32m    560\u001B[0m         \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    561\u001B[0m             result = _VF.lstm(input, batch_sizes, hx, self._flat_weights, self.bias,\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "from model.LSTM import LSTM\n",
    "\n",
    "model = LSTM(input_size=features.shape[1], output_size=4)\n",
    "loss_function = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(),\n",
    "                       lr=0.01, weight_decay=5e-4)\n",
    "print(model)\n",
    "print(\"---------------\")\n",
    "# print(features.shape[1])\n",
    "\n",
    "print(labels.dtype)\n",
    "print(features.shape)\n",
    "print(features.shape[0])\n",
    "result = []\n",
    "for i in range(200):\n",
    "    for j in range(features.shape[0]):\n",
    "        t = time.time()\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        x= features[j]\n",
    "        x = np.expand_dims(x, axis=0)\n",
    "        # print(x.shape)\n",
    "        x = np.expand_dims(x, axis=0)\n",
    "        x = torch.Tensor(x)\n",
    "        output = model(x)\n",
    "        output = F.softmax(output, dim=0)\n",
    "        # output = nn.Softmax(output, dim=1)\n",
    "        # loss = loss_function(output, labels[j])\n",
    "        # loss = F.nll_loss(output, labels[j])\n",
    "        # loss.backward()\n",
    "        # result.append(output)\n",
    "        # loss_train = F.nll_loss(output[idx_train], labels[idx_train])\n",
    "        # loss_train.backward()\n",
    "        if j%2000 ==1:\n",
    "            print(output)\n",
    "        optimizer.step()\n",
    "\n",
    "    # if i%25 == 1:\n",
    "    #     print(f'epoch:{i:3} loss:{loss.item():10.8f}')\n",
    "\n",
    "print(result)\n",
    "print('02_ok')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.7750275  0.5118148  0.2948432 ]\n",
      " [0.753338   0.1481781  0.44358945]\n",
      " [0.00553626 0.15809679 0.57791334]\n",
      " [0.17234921 0.28322703 0.14579862]\n",
      " [0.6272144  0.01666093 0.46982068]\n",
      " [0.2781887  0.84367704 0.9976265 ]\n",
      " [0.90245086 0.4840855  0.882231  ]\n",
      " [0.40373337 0.39092463 0.33362412]\n",
      " [0.48373854 0.8461616  0.54425985]\n",
      " [0.23575747 0.895375   0.30632824]]\n",
      "tensor([[0.8579, 0.8337, 0.0000, 0.1860, 0.6931, 0.3040, 1.0000, 0.4440, 0.5332,\n",
      "         0.2567],\n",
      "        [0.5635, 0.1497, 0.1610, 0.3034, 0.0000, 0.9412, 0.5319, 0.4259, 0.9440,\n",
      "         1.0000],\n",
      "        [0.1750, 0.3496, 0.5073, 0.0000, 0.3804, 1.0000, 0.8645, 0.2205, 0.4678,\n",
      "         0.1885]])\n",
      "tensor([[ 1.1109,  1.0335, -1.6351, -1.0398,  0.5834, -0.6621,  1.5656, -0.2141,\n",
      "          0.0714, -0.8136],\n",
      "        [ 0.1790, -1.0264, -0.9935, -0.5788, -1.4624,  1.2791,  0.0871, -0.2218,\n",
      "          1.2873,  1.4504],\n",
      "        [-0.8103, -0.2217,  0.3099, -1.4001, -0.1179,  1.9708,  1.5141, -0.6568,\n",
      "          0.1767, -0.7648]])\n",
      "03_ok\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch as th\n",
    "import scipy.sparse as sp\n",
    "\n",
    "a = th.rand(10,3)\n",
    "\n",
    "# rowsum = np.array(a.sum(1))\n",
    "# r_inv = np.power(rowsum, -1).flatten()\n",
    "# r_inv[np.isinf(r_inv)] = 0.\n",
    "# r_mat_inv = sp.diags(r_inv)\n",
    "# mx = r_mat_inv.dot(a)\n",
    "# print(mx)\n",
    "\n",
    "a = np.array(a)\n",
    "print(a)\n",
    "x1 = []\n",
    "x2 = []\n",
    "for i in range(a.shape[1]):\n",
    "    temp = (a[:, i] - np.min(a[:, i])) / (np.max(a[:, i]) - np.min(a[:, i]))\n",
    "    x1.append(temp)\n",
    "\n",
    "for i in range(a.shape[1]):\n",
    "    temp = (a[:, i] - np.mean(a[:, i])) / np.std(a[:, i])\n",
    "    x2.append(temp)\n",
    "\n",
    "x = th.Tensor(x1)\n",
    "print(x)\n",
    "x = th.Tensor(x2)\n",
    "print(x)\n",
    "\n",
    "\n",
    "print('03_ok')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}